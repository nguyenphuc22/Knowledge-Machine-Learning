# Cây Quyết Định

Cây quyết định (Decision Tree) là một mô hình học máy được sử dụng cho cả phân loại (classification) và hồi quy (regression). Nó hoạt động dựa trên cấu trúc cây, nơi mỗi nút biểu diễn một quyết định hoặc một phân nhánh dựa trên một thuộc tính của dữ liệu, và mỗi là của cây đại diện cho một kết quả hoặc dự đoán.

## Định nghĩa

- **Cấu trúc Cây**: Cây quyết định bắt đầu từ một nút gốc và mở rộng ra các nhánh dựa trên các quyết định liên quan đến thuộc tính của dữ liệu.
- **Nút và Lá**: Mỗi nút trên cây đặt một câu hỏi về một thuộc tính, và mỗi nhánh từ nút đó tương ứng với một trong các câu trả lời có thể. Các nút cuối cùng, không có nhánh con, được gọi là lá và chứa dự đoán hoặc kết quả.

## Khái niệm

- **Phân loại và Hồi Quy**: Trong phân loại, cây quyết định dùng để xác định nhãn lớp cho dữ liệu, còn trong hồi quy, nó dự đoán giá trị số.
- **Phân chia Dựa trên Thuộc tính**: Các quyết định về cách phân chia dữ liệu tại mỗi nút thường dựa trên các phép đó như Gini Impurity, Entropy hoặc Information Gain.
- **Tính Trực quan và Dễ Hiểu**: Một ưu điểm lớn của cây quyết định là chúng rất trực quan và dễ hiểu, ngay cả đối với những người không chuyên về học máy.
- **Overfitting**: Một thách thức khi sử dụng cây quyết định là chúng dễ dàng overfitting, tức là mô hình quá phù hợp với dữ liệu huấn luyện và không tổng quát hóa tốt trên dữ liệu mới.
- **Underfitting**: xảy ra khi cây không đủ phức tạp để nắm bắt các mô hình cơ bản hoặc các quy luật trong dữ liệu. Điều này thường xảy ra khi mô hình quá đơn giản so với độ phức tạp của dữ liệu thực tế mà nó cần học.

## Ứng dụng

Cây quyết định được ứng dụng rộng rãi trong nhiều lĩnh vực từ y khoa, tài chính, marketing, đến các hệ thống khuyến nghị và nhiều hơn nữa, nhờ khả năng cung cấp quyết định dựa trên dữ liệu một cách rõ ràng và dễ hiểu.

## Ví dụ về Overfitting

Hãy tưởng tượng bạn đang xây dựng một mô hình học máy để dự đoán giá nhà dựa trên diện tích và vị trí. Trong trường hợp của overfitting:

- **Trong Dữ Liệu Huấn Luyện**: Mô hình có thể học rất tốt từ dữ liệu huấn luyện, thậm chí nắm bắt cả những chi tiết nhỏ như một ngôi nhà cụ thể có giá cao vì nó gần một cây cổ thụ đẹp.
- **Trong Dữ Liệu Thực Tế**: Khi mô hình này được áp dụng cho dữ liệu mới, nó sẽ không hoạt động tốt vì không phải tất cả các ngôi nhà gần cây cố thụ đều đắt. Mô hình đã học "quy tắc" không chính xá từ dữ liệu huấn luyện.

### **Cây bị overfitting**

```mermaid
graph TD
    A[Start] -->|Feature1 > 5| B[Feature2 < 3]
    A -->|Feature1 ≤ 5| C[Feature2 > 3]
    B -->|Feature3 ≤ 2| D[Class: 1]
    B -->|Feature3 > 2| E[Class: 0]
    C -->|Feature4 ≤ 1| F[Feature5 < 4]
    C -->|Feature4 > 1| G[Feature5 ≥ 4]
    F -->|Feature6 ≤ 3| H[Class: 0]
    F -->|Feature6 > 3| I[Class: 1]
    G -->|Feature7 ≤ 2| J[Class: 1]
    G -->|Feature7 > 2| K[Class: 0]
```

### **Cây không bị overfitting**
```mermaid
graph TD
    A[Start] -->|Feature1 > 5| B[Feature2 < 3]
    A -->|Feature1 ≤ 5| C[Feature2 > 3]
    B -->|Feature3 ≤ 2| D[Class: 1]
    B -->|Feature3 > 2| E[Class: 0]
    C --> D
    C --> E
```

Nên biểu diễn dưới dạng sơ đồ để dễ hiểu hơn. Đây là Ví dụ khác.

![img.png](Image/img.png)

Cắt nó đi

![img_1.png](Image/img_1.png)

### Nguyên Nhân của Overfitting

- **Dữ liệu Huấn Luyện Có Hạn**: Nếu dữ liệu huấn luyện không đủ đa dạng hoặc quá ít, mô hình có thể "học thuộc lòng" chúng thay vì học cách tổng quát hóa
- **Mô Hình Quá Phức Tạp**: Mô hình với quá nhiều tham số (như một mạng lưới nơ-ron sâu và phức tạp) có thể nắm bắt quá mức chi tiết trong dữ liệu huấn luyện.
- **Không Có Hạn Chế về Độ Sâu của Cây**: Nếu không có hạn chế về độ sâu, cây có thể phát triển quá mức ần thiết để phản ánh chính xác các mẫu trong dữ liệu huấn luyện.

## Giới hạn độ sâu

Giới hạn độ sâu của cây quyết định là một kỹ thuật được sử dụng trong học máy để ngăn chặn việc overfitting của cây quyết định. Độ sâu của cây quyết định chỉ số lượng tầng hoặc cấp của nút từ nút gốc đến nút lá sâu nhất. Đặt giới hạn cho độ sâu này có những tác động sau:

### Mục Đích
- **Ngăn Chăn Overfitting**: Giới hạn độ sâu giúp ngăn chặn việc mô hình học quá mức từ dữ liệu huấn luyện, bao gồm cả nhiễu và các chi tiết không quan trọng.
- **Tăng Khă Năng Tổng Quát Hoá**: Bằng cách giới hạn độ sâu, cây quyết định có thể tổng quá  hoá tốt hơn trên dữ liệu chưa từng thấy.

### Cách Thức Hoạt Động

- **Định Nghĩa Độ Sâu Tối Đa**: Trước khi xây dựng cây, một giá trị độ sâu tối đa được định nghĩa. Cây sẽ không được phép phát triển quá giới hạn này.
- **Cắt Tỉa (Pruning)**: Trong quá trình hoặc sau khi xây dựng cây, các nút và nhánh có thể được cắt bỏ (pruning) để đảm bảo cây không vượt quá độ sâu đã định.

### Lợi ích

- **Tránh Phức Tạp Không Cần Thiết**: Giới hạn độ sâu giúp đơn giản hoá mô hình, tránh việc xây dựng cây quá phức tạp mà không cần thiết.
- **Tăng Tốc Độ Huấn Luyện và Dự Đoán**: Cây nhỏ hơn, ít phức tạp hơn thường nhanh hơn trong quá trình huấn luyện và dự đoán.

### Lưu ý

- **Cân Nhắc Lựa Chọn Độ Sâu**: Độ sâu tối ưu phụ thuộc vào bản chất của dữ liệu và mục tiêu của mô hình. Độ sâu quá nhỏ có thể dẫn đến underfitting, nơi mô hình không đủ phức tạp để hiểu dữ liệu.
- **Sử Dụng Kết Hợp Các Kỹ Thuật Khác**: Thường được sử dụng cùng với các kỹ thuật khác như min samples split, min samples leaf, và pruning để đạt hiệu suất tối ưu.

## Cây bị underfitting

Underfitting trong mô hình Cây Quyết Định xảy ra khi cây không đủ phức tạp để nắm bắt các mô hình cơ bản hoặc các quy luật trong dữ liệu. Điều này thường xảy ra khi mô hình quá đơn giản so với độ phức tạp của dữ liệu thực tế mà nó cần học.

### Đặc Điểm Underfitting

- **Không Nắm Bắt Được Xu Hướng Dữ Liệu**: Cây quyết định không đủ sâu hoặc không có đủ nhánh để hiểu đầy đủ các mối quan hệ và mô hình trong dữ liệu.
- **Hiệu Suất Kém Trên Cả Dữ Liệu Huấn Luyện và Kiểm Thử**: Mô hình không hoạt động tốt trên cả dữ liệu huấn luyện và dữ liệu kiểm thử, thể hiện rằng nó không học đủ từ dữ liệu.

### Nguyên Nhân

- **Mô Hình Quá Đơn Giản**: Mô hình có thể không có đủ các nhánh hoặc độ sâu để hiểu các mô hình phức tạp trong dữ liệu.
- **Giới Hạn Độ Sâu Quá Thấp**: Đặt giới hạn độ sâu quá thấp cho cây quyết định có thể ngăn cản việc hình thành một mô hình đầy đủ.
- **Dữ Liệu Huấn Luyện Không Đủ**: Số lượng hoặc đa dạng của dữ liệu huấn luyện không đủ để mô hình học một cách hiệu quả.

### Hậu Qủa

- **Khả Năng Tổng Quát Hoá Kém**: Mô hình không thể áp dụng những gì đã học từ dữ liệu huấn luyện để dự đoán chính xác trên dữ liệu mới.
- **Không Phát Hiện Được Mẫu Quan Trọng**: Mô hình có thể bỏ lỡ những mẫu quan trọng hoặc các quy luật cơ bản trong dữ liệu.

### Cách Phòng Tránh Underfitting

- **Tăng Độ Sâu của Cây**: Cho phép cây phát triển đủ độ sâu để nắm bắt được sự phức tạp của dữ liệu.
- **Xem Xét Lại Dữ Liệu Huấn Luyện**: Đảm bảo rằng dữ liệu huấn luyện đủ đa dạng và phong phú.
- **Kiểm Tra Mô Hình**: Sử dụng cross-validation và các phép đo hiệu suất khác để đánh giá xem liệu mô hình có đang underfit hay không.

## So sánh Entropy, Information Gain và Gini Impurity

### Entropy

- **Ưu điểm**:
    - Tốt trong việc đo lường mức độ hỗn loạn hay không chắc chắn trong dữ liệu.
    - Thích hợp cho việc phân loại cân đối giữa các lớp
- **Nhược điểm**:
    - Tính toán phức tạp hơn do cần tính logarit.

### Information Gain

- **Ưu điểm**:
    - Hiệu quả trong việc đánh giá sự cải thiện mà một thuộc tính mang lại cho việc phân loại.
    - Dựa trên Entropy, nó cung cấp cái nhìn sâu sắc về lượng thông tin thu được từ một thuộc tính.
- **Nhược điểm**:
    - Có thể thiên vị với các thuộc tính có nhiều giá trị khác nhau.
    - Có thể dẫn đến overfitting nếu không được sử dụng cẩn thận.

### Gini Impurity

- **Ưu điểm**:
    - Tính toán đơn giản và nhanh chóng hơn so với Entropy.
    - Hiệu quả trong việc phân chia dữ liệu, đặc biệt khi số lượng lớp không quá lớn.
- **Nhược điểm**:
    - Có thể không hiệu quả như Entropy trong một số trường hợp phức tạp hoặc khi có nhiều lớp phân loại.

### Nhận Xét Chung

- **Chọn lụa phương pháp**: Lựa chọn giữa Gini Impurity và Entropy/Information Gain thường phụ thuộc vào các yêu cầu cụ thể của bài toán và dữ liệu. Trong nhiều trường hợp, sự khác biệt giữa chúng không đáng kể về mặt hiệu suât.
- **Độ chính xác và hiệu suất**: Trong khi Entropy và Information Gain có thể cung cấp cái nhìn sâu sắc và chính xác, Gini thường được ưa chuộng vì tính toán nhanh và đơn giản.
- **Cân nhắc về dữ liệu**: Khi làm việc với dữ liệu có nhiều thuộc tính hoặc các thuộc tính có nhiều giá trị khác nhau, cần cân nhắc kỹ lưỡng về sự lựa chọn giữa Information gain và Gini để tranh thiên vị hoặc overfiting.
